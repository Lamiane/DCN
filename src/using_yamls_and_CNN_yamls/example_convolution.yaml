!obj:pylearn2.train.Train { # our experiment
    "dataset": &src !pkl: "${PYLEARN2_DATA_PATH}/cifar10/pylearn2_whitened/train.pkl", # path to the dataset pkl file

    "model": !obj:pylearn2.models.rbm.GaussianBinaryRBM { # our model: Gaussian Binary Restricted Boltzman machine

        "vis_space" : &vis_space !obj:pylearn2.space.Conv2DSpace { # nth in documentation, probably the shape of input
                "shape" : [32,32], # rows, cols
                "num_channels" : 3 # number of channels
        },
        "hid_space" : &hid_space !obj:pylearn2.space.Conv2DSpace { # nth in documentation, probably the shape of input
                "shape" : [27,27],  # rows, cols
                "num_channels" : 10 # number of channels TODO: why?
        }, # todo: how does it work? Read about RBMs.
        "transformer" : !obj:pylearn2.linear.conv2d.make_random_conv2D {    # nth in documentation TODO: find out!
                                                                            # Creates a Conv2D with random kernels
                                                                            # kernels are random, are their values
                                                                            # adapted during learning?
                "irange" : .05, # TODO: find out
                "input_space" : *vis_space,     # what shall be convoluted
                "output_space" : *hid_space,    # where shall it be stored
                "kernel_shape" : [6,6],         # shape of convolution matrix
                "batch_size" : &batch_size 5    # batch size
        },
        "energy_function_class" : !obj:pylearn2.energy_functions.rbm_energy.grbm_type_1 {}, # http://deeplearning.net/software/pylearn2/library/energy_functions.html?highlight=energy_functions#pylearn2.energy_functions.rbm_energy.GRBM_Type_1
                                                                                # This GRBM energy function is designed
                                                                                # to make it easy to interpret score
                                                                                # matching as being a denoising
                                                                                # autoencoder.
        "learn_sigma" : True,   # "bool, optional"
        "init_sigma" : .3333,   # initial sigma value
        "init_bias_hid" : -2.,  # Initial value for the biases on hidden units.
        "mean_vis" : False,     # "Don’t actually sample visibles; make sample method simply return mean." TODO: understand
        "sigma_lr_scale" : 1e-3 # "float, optional" - oh, thank you, documentations! you're so helpful. TODO: find that out

    },

    "algorithm": !obj:pylearn2.training_algorithms.sgd.SGD {

        "learning_rate" : 1e-4, # The learning rate to use. Train object callbacks can change the learning rate after
                                # each epoch. SGD update_callbacks can change it after each minibatch.
        "batch_size" : *batch_size, # The size of the batch to be used. If not specified, the model will be asked for
                                    # the batch size, so you must have specified the batch size there. (Some models are
                                    # rigidly defined to only work with one batch size)
        "batches_per_iter" : 20,    # The number of batches to draw from the iterator over training examples. If
                                    # iteration mode is ‘sequential’ or ‘shuffled_sequential’, this is unnecessary;
                                    # when unspecified we will iterate over all examples.
        "monitoring_batches" : 20,  # At the start of each epoch, we run “monitoring”, to evaluate quantities such as
                                    # the validation set error. monitoring_batches, if specified, determines the number
                                    # of batches to draw from the iterator for each monitoring dataset. Unnecessary if
                                    # not using monitoring or if monitor_iteration_mode is ‘sequential’ and batch_size
                                    # is specified (number of batches will be calculated based on full dataset size).
        # We monitor how well we're doing during training on a validation set
        # If not specified, no monitoring is used. If specified to be a Dataset, monitor on that Dataset.
        # If specified to be dictionary, the keys should be string names of datasets, and the values should be Datasets.
        "monitoring_dataset" : !obj:pylearn2.datasets.dense_design_matrix.from_dataset {    # Constructs a random subset
                                                                                            # of a DenseDesignMatrix
                "dataset" : *src,       # dataset defined in the first line
                "num_examples" : 100    # number of examples in created subset
        },

        "cost" : !obj:pylearn2.costs.ebm_estimation.SMD { # TODO: ogarnac calosc

            "corruptor" : !obj:pylearn2.corruption.GaussianCorruptor {
                    "stdev" : 0.4
            },
        },

        "termination_criterion" : !obj:pylearn2.termination_criteria.MonitorBased { # stoping based on monitor
            "prop_decrease" : 0.01, # The threshold factor by which we expect the channel value to have decreased
            "N" : 10, # Number of epochs to look back
        }
       },
      # A TrainExtension that uses the on_monitor callback to adjust the learning rate on each epoch. It pulls out a
      # channel from the model’s monitor and adjusts the learning rate based on what happened to the monitoring channel
      # on the last epoch. If the channel is greater than high_trigger times its previous value, the learning rate will
      # be scaled by shrink_amt (which should be < 1 for this scheme to make sense). The idea is that in this case the
      # learning algorithm is overshooting the bottom of the objective function.
      "callbacks" : [!obj:pylearn2.training_algorithms.sgd.MonitorBasedLRAdjuster {
                "max_lr" : .01 # All updates to the learning rate are clipped to be at most this value.
                } ],

    "save_path": "${PYLEARN2_TRAIN_FILE_FULL_STEM}.pkl", # here the model will be stored
    "save_freq": 1  # Frequency of saves, in epochs. A frequency of zero disables automatic saving altogether.
                    # A frequency of 1 saves every epoch.
}


