% !TEX encoding = UTF-8 Unicode
%ANG: jak się wstawia przecinki w zdaniach, synonimy do usually
%ANG: kiedy przed ed podwaja się spółgłoskę?
%ANG: synonyms do ``as stated above'' ?

\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage[toc,page]{appendix}
\graphicspath{ {images/} }
\usepackage{parskip}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[color=yellow]{todonotes}
\newcommand{\igor}[1]{\textcolor{OrangeRed}{#1}}
\usepackage{url}

\newenvironment{bottompar}{\par\vspace*{\fill}}{\clearpage}

\title{Praca Magisterska}
\author{Agnieszka Pocha}

\pagenumbering{gobble}
\begin{document}
  \begin{center}
    % Title Page
    Uniwersytet Jagielloński
    
    Wydział Matematyki i Informatyki
    
    INSTYTUT INFORMATYKI i MATEMATYKI KOMPUTEROWEJ
    
    Studia stacjonarne 
  \end{center}
  \vspace{\stretch{1}}
  Nr indeksu: 1064077  
  
  \begin{center}
   Agnieszka Pocha 
   \vspace{\stretch{0.1}}
   
   \large Convolutional Neural Networks for Drug Design 
  \end{center}
  \vspace{\stretch{1}}
  \begin{flushright}
    Opiekun pracy magisterskiej:
    
    dr hab. Igor Podolak 
  \end{flushright}

  \begin{bottompar}
  \begin{center}
   \large Kraków 2015
  \end{center}
  \end{bottompar}

  
  %\maketitle
  \begin{abstract}
    The goal of this work is to build a model that would classify ligands as active or inactive based on their fingerprints. Convolutional neural network are used. Two approaches are described. Two new (?) methods to include unlabeled samples in the learning procedure are presented.
  \end{abstract}
  
%  \frontmatter
  \pagenumbering{roman}
  \tableofcontents\vfill\eject
    
  
%  \mainmatter
  \pagenumbering{arabic}
  \chapter{Introduction}
      There is a constant need for novel drugs to be designed, either because new diseases are found, old drugs proove to be ineffective, there are patents on current drugs, \textit{etc}. This might be a lengthy and very costly process, even if high-throughput methods of \textit{in vitro} (using cultures of cells) screening are used~\cite{Breda2008}. An alternative is the \textit{virtual screening} (frequently called \textit{in silico} screening, \textit{i.e.} screening conducted within a computer chip). The approach is to check, using computer methods, if a small chemical molecule (a ligand) is \textit{active} with a given protein, \textit{i.e.} it either activates or prohibits its functioning, that is connected with some disease. Molecules found to be particularly likely to be active are then passed to wet-laboratory tests. This forms a pipeline filtering out most of possible molecules (the chemical space size is in the range of $10^{60}$!) leaving just a few. In usual applications starting from a million of molecules, the virtual screening (together with some toxicity checking and drug-likeliness expert's methods) ends up in less than 10 molecules to test in a laboratory (\textit{e.g.}~\cite{Pethukov2006}).

	  The virtual screening might be done in, basically, two ways~\cite{Geppert2010,Varnek2012}
	  \begin{itemize}
      \item target- (or structure- or protein-) based screening when the structure of a protein is known and methods of \textit{docking} might be used to predict the activity,
      \item ligand-based screening if protein structure is \textit{not} known, but there is a knowledge about some molecules that are active; then the task is to find molecules similar to those active (following a Johnson-Maggiora rule that similar compounds should have similar properties~\cite{Varnek2012}).
      \end{itemize}
  
    \section{The goal of this paper}
    
     Virtual screening involves checking great amounts of molecules in order to find those which have interesting features. In this paper we describe how convolutional neural networks can be used to approach this problem. Our goal was to build a model that would and we proposed two approaches to tackle this problem. Moreover, we present new approach to make use of unlabeled examples during the learning process of convolutional neural networks.
    
    \section{Related work}
    % TODO
    During last few years research was conducted in the area on incorporating machine learning methods to address the problem of virtual screening~\cite{kernel_biology, evaluation2007, MLforINSilicio}. Such methods as support vector machines~\cite{SVM}, binary kernel disctimination~\cite{BKD} or neural networks~\cite{NN_VS} have been successfully used to address this problem. In this paper we show that using convolutional neural networks is also a promising approach.

    \section{Problem}\label{sec:PROBLEM}
    The task to be solved here is a kind of target-based virtual screening. Protein-molecule interactions are given along with labels describing which of these pairs result in activation. The data is obtained with a docking approach.
    
    First the molecules are docked in a protein and then the molecules' pharmacophore features are calculated. Based on the docking adjustment the score is calculated for each molecule. If the score is above a certain threshold, the complex is classified as active. There is also another threshold, below which the complex is classified as inactive. If the score for a molecule is in between these two thresholds, the molecule is classified as middle (or unclassified). The machine learning task is to predict which molecules activate (or prohibit) the protein.
    
    \begin{figure}[h!]
	  \centering
	  \includegraphics[scale=0.2]{docked.png}
	  \caption{A molecule (blue) with a docked ligand(orange)~\ref{docked}.}
	  \label{docked}
    \end{figure} 
    
    \emph{Proteins} are molecules built from \emph{amino acid residues} forming a single \emph{chain}. Some proteins have less than 100 residues while others might have even few thousands of them. The order of amino acids in the chain and their spatial arrangement carry a lot of information. 
    
    Our goal was to build a model that would classify the complex as \emph{active} or \emph{inactive} based on its \emph{fingerprint}. Fingerprints are a widely used representation of molecules. There are many types of fingerprints designed. They vary in length and the type of information they carry. Some of them are designed for specific tasks. (citation needed). Conventionally, fingerprints represent a molecule as a vector. Each element of this vector describes whether a specific structure is present in the molecule or not, e.g. whether the molecule has a hydrophobic group or not. 
        
    Vectors are commonly used in machine learning as data representation because they can be stored in matrices which allows easy calculations. Even though representing a protein as a vector means loosing a lot of information about its topology, it enables effective computation and still provides us with reasonable results. 
    
    As already said, there are many different fingerprints designed. They vary in length and the features included. In this paper we used 2D-SIFt~\cite{2DSIFT} fingerprints. We used convolutional neural networks because they are eligible to work with data which topological order carries information.
     
     
    \section{2D-SIFt fingerprints}
    In this work the 2D-SIFt fingerprints are used~\cite{Mordalski2011, cos-jeszcze}. They are a composition of Structural Interaction Fingerprints (SIFt)~\cite{Singh2006} together with the pharmacophore model. SIFts code information from the point of view of the receptor conveying \textit{how} a molecule interacts with the protein, not \textit{what} interacts. A single block codes information in a nine element long binary vector:
    \begin{align*}
    [\text{any contact},&\text{backbone},\text{sidechain},\text{polar},\text{hydrophobic}, \\
    &\text{H-bond acceptor},\text{H-bond donor},\text{aromatic},\text{charged}
    ].
    \end{align*}
    These are informations about the single points of contact between the target protein and the molecule.
    
    Additionaly, there is a pharmacophore model of the molecule added. A pharmacophore is a set of sferic and electronic features that is necessary to ensure the optimal supramolecular binding with a protein (or another specific biological target structure) and to trigger (or to block) its biological response~\cite{GLOSSARY}.
    
    One can see pharmacophore features as a way to perform the virtual screening in a half way between protein- and ligand-based approach. The docking needs to be performed for plenty but not all of the molecules. At first, the pharmacophore features of the docked molecules are computed. Afterwards, a big data set of molecules with known pharmacophore features can be screened in order to find molecules most similar to those which were docked and classified as active. The protein itself is used only at the beginning during the process of docking. Later it might be dismissed as the whole method relies on pharmacophore features.
    
    In 2D-SIFt each pharmacophore point is described using a six element vector:
    \begin{align*}
    [\text{H-bond donor},&\text{H-bond acceptor},\text{hydrophobic}, \\
    &\text{negatively charged},\text{positively charged},\text{aromatic}].
    \end{align*}



    These two models combined result in a $6\times{}9$ matrix for each possible point of contact. The model shall now be called 2D-SIFt~\cite{Mordalski2011}. For a given protein and given molecule this results in a string of $n$ of $6\times{}9$ arrays glued together, where $n$ describes the number of residues of a given protein. We can say, that this representation resembles and image, on which patterns shall be looked after. This explains why a convolutional network is used in this work.
      
   
    \section{How this paper is organised}
    
    At first, the convolutional neural networks are described. We present such concepts as convolution or pooling. The properties of convolutional neural networks are given. 
    
    Chapter \ref{chap:OUR_MODEL} is the most important part of the paper. In this chapter we present the two approaches that we used to address the problem. We describe in detail the data on which we performed experiments and the preprocessing that we used to extract most interesting features from the data. A detailed explanation of the architecture that we used is given. We describe in detail both training methods that we created to tackle the problem of using unlabeled samples in the learning algorithm - the naive approach and the fancy approach. 
    
    Our results are described in the next chapter. Finally, in the last chapter we discuss possible ways to further develop our approach. At the end of this paper there is a short appendix which includes some less important concepts used in this work.
    

  \chapter{Convolutional Neural Networks}
  In this chapter the Convolutional Neural Network (CNN) model shall be described.
  
      The definition of a convolutional neural network is: a neural network that takes advantage of using the convolution operation. CNN might be seen as a network consisting of two parts - first part is the convolutional part and it is responsible for extracting the features from the data. The second part might be a softmax layer responsible for classifying samples based on features provided by the convolutional part. This schema is shown on figure \ref{fig:con_network}. 
      
     \begin{figure}[h!]
	  \centering
	  \includegraphics[width=\textwidth]{CNNArchitecture.jpg}
	  \caption{An example of convolutional neural network~\cite{CONV_NET}.}
	  \label{fig:con_network}
	\end{figure} 
      
      In this section we will show what motivation stands behind using convolutional neural networks, explain what is the convolution operation, and what is pooling. Types of activation functions that might be used in convolutional neural networks
      will be presented as well as the learning algorithm. 
      
      \section{Motivation}
	Convolutional neural networks take advantage of data which topological order carries some information. Therefore, CNNs are often applied to image recognition or natural language processing problems and achieve state-of-the-art results~\cite{ImageNet, CONV_NLP}. Since the the topological order of the data we are working on carries information (due to how the proteins are built), we expect that using convolutional neural networks might bring good results. 
	
      \section{Computation Flow}
	As stated above, CNNs can be conceptually divided into two subnetworks. In this subsection it will be described how the data is processed within the convolutional subnetwork. Not much attention will be put to classifying subnetwork as there is a wide variety of possible approaches. 
	
	There are three elemental operations that are performed in each layer of the convolutional subnetwork. At first, the input is convoluted with a convolution matrix. The result of this operation becomes an input to the activation function, which output becomes the input to the pooling function. The output of the pooling function becomes the input to the next layer which might be another convolutional layer and the whole process will begin again. This process is schematically depicted on figure \ref{fig:con_neur} and it can be also described by the following formula: 
	
	\begin{center}
	$output = p(\sigma(c(input)))$, 
	\end{center}
	
	
	where $c$ is the convolution function, $\sigma$ is the activation function, and $p$ is the pooling function. 
	
	\begin{figure}[h!]
	  \centering
	  \includegraphics[width=\textwidth]{convolutional_neuron.png}
	  \caption{The three elemental operations performed in each convolutional layer.}
	  \label{fig:con_neur}
	\end{figure} 
	
	\begin{figure}[h!]
	  \centering
	  \includegraphics[width=\textwidth]{mylenet.png}
	  \caption{An example of a whole neural network with stages of convolution and polling marked~\cite{con_neur_INT}.}
	  \label{fig:con_neur_PIC}
	\end{figure} 
	One might also imagine three consecutive seperate layers: a convolutional layer, a classical layer that applies activation function and finally a pooling layer. 
	
	Each of these operations will be described in details in the following subsections. 
	
	\subsection{Convolution}\label{sec:convolution}
	Convolution operation takes as operands two functions and returns a new function as a result. Mathematically, convolution is defined as:  
	\begin{center}
	$c(t) = \int\limits_{-\infty}^\infty f(x)g(t-x)dx$, 
	\end{center}
	
	where $c$ is a function returned by convolution operation and $t$ is a point in which function $c$ is evaluated. $c$ is defined as an integral over two other functions: $f$ is often called the input, while $g$ is often reffered to as a weighting function. 
	
	Convolution might be seen as a measure how well two functions fit. Function $g$ is a pattern that moves along function $f$ and it each place it measures how well the two functions fit. If the similarity is high, the returned value is high as well. 
		
	\subsubsection{Convolution for neural networks}
	  
	  It is worth considering how this equation can be applied to neural networks. Since representing real values in the computers is rather troublesome, it is desirable to express this equation in a discrete form, which is shown below: 
	  \begin{center}
	  $\overline{c}(t) = \sum\limits_{x = -\infty}^\infty \overline{f}(x)\overline{g}(t-x)$  
	  \end{center}
	  
	  If we want to apply this equation to neural networks we might want to think of $\overline{f}$ as a data sample. In such approach $\overline{g}$ would become a matrix that is moving around the data sample producing a single value in each place. From now on, we will call $f$ an input, $g$ a filter or a convolution matrix or a convolution window and $c$ an output. 
	  
	  Usually, the convolution window is much smaller then the input and convolution is appplied multiple times. Each time a  submatrix of input is multiplied by the convolution window. The result of this operation is a scalar, and a result of a whole process is a matrix. Each layer of neural network might include multiple filters and thus each layer might produce an output of higher dimensionality than the provided input - the additional dimension is produced due to using multiple filters. 
	  
	  It is worth noting, that there are different filters in each layer. Filters in the next layer work on the features extracted from the previous layers. Therefore, the later the layer is in the neural network, the more complicated patterns it may discover. 
	  
	  \subsubsection{Size and stride, spatial invariance}
	  
	  It is worth taking a closer glance at how convolution works in details. First, we have to define two important parameters of the convolution windows - their size and stride. The size is simply the size of the convolution matrix. The stride defines which part of the input will be convoluted next with respect to the part of the input that is currently being convoluted. This concept is shown in figure \ref{fig:convolution_size_and_stride}. 
	  
	  
	  \begin{figure}[h!]
	    \centering
	    \includegraphics[scale=0.65]{convolution_size_and_stride.png}
	    \caption{The figure shows three consecutive positions of a convolution window - yellow, blue and red. The input is of size $4\times5$, the convolution window has size $2\times3$ and stride $1\times2$, which means the window can move two elements to the right or one to the bottom.}
	    \label{fig:convolution_size_and_stride}
	  \end{figure} 
	  
	  Three consecutive positions of a convolution window are depicted. Each position defines which submatrix of the input will be multiplied by the convolution matrix. Each multiplication will produce a single value - these values when combined will produce an output matrix. 
	  
	  It should be noted that each time the convolution matrix is the same, only the part of the input that is being convoluted changes. Each filter is specialised in finding a specific pattern - because it is convoluted with many submatrices of the input it will find that pattern regardless of where the patter is on the input matrix. This property is called spatial invariance. Figure \ref{fig:spatial_invariance} shows this concept more clearly. 
	  	  
	  \begin{figure}[h!]
	    \centering
	    \includegraphics[scale=0.5]{spatial_invariance.png}
	    \caption{In the upper row two locations of a certain feature are shown in red. In the bottom row the location of a convolution window that will discover this feature is shown in yellow. As can be observed, by moving the convolution window it is possible to extract a certain feature regardless of its location.}
	    \label{fig:spatial_invariance}
	  \end{figure} 
	  
	  
	  As stated above, in each layer usually there are plenty of convolution matrices - each is specialised in finding a specific pattern. Each pattern might be seen as a useful feature of the data and convolution provides us with some sort of map that shows where in the input this feature exists and where it does not. Therefore, it is often useful to see convolution windows as filters or feature extractors. 
	  
	  \subsubsection{Data size reduction due to convolution operation}
	  
 	  Let the input be an $I\times J$ matrix and the convolution filter a $K\times L$ matrix with a $1\times1$ stride. Therefore, the first submatrix to multiply by the convolution filter will be $[1:K]\times[1:L]$ and it will return a single value. The last submatrix will be $[I-K+1:I]\times[J-L+1:J]$ and it will also produce a  single value. As a result the output will be a $[I-K+1]\times[J-L+1]$ matrix. This process is shown on picture \ref{fig:convolution_size_reduction}. 
	  
	  
	  \begin{figure}[h!]
	    \centering
	    \includegraphics[scale=0.5]{convolution_size_reduction.png}
	    \caption{In this picture $I = 3, J = 9, K$ and $L = 2$. First submatrix and the output it produces are shown in yellow, second are shown in blue and the last are shown in green. The dimensionality reduction might be observed.}
	    \label{fig:convolution_size_reduction}
	  \end{figure} 
	  
	  It can be observed that the values laying close to the edge of the input matrix will be underrepresented. The upper left corner of the input matrix will have influence on only one element of the output (the yellow one) while its right neighbour will already have influence on two elements of the output (the yellow and the blue one). The elements in the middle of the input matrix will influence four elements of the output. Such unequalities of the influence might be undesirable. There is a variety of ways to address this problem, e.g. zero-padding, but we will not cover them. More information about such methods can be found in \cite{Bengio}. 
	  
	\subsection{Activation function}\label{sec:ACTIVATION_FUNCTIONS}
	  There are few possible activation functions that might be used in convolution neural networks~\cite{DUTCH}.
	  \begin{itemize}
	   \item the sigmoid activation function $\sigma(x) = \frac{1}{1 + e^{-x}}$
	   \item the hyperbolic tangent activation function $tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}$
	   \item the rectifier linear function~\cite{GLOROT_BENGIO} $rect(x) = max(0,x)$
	  \end{itemize}

	
	\subsection{Pooling}
	  Pooling is an operation, usually applied to a matrix, that takes as an input multiple values and returns a single value describing the input. Typical pooling functions are~\cite{DUTCH}:
	  \begin{itemize}
	    \item max pooling - the max value of input is returned
	    \item average pooling - the average value of input is returned
	    \item stochastic max pooling - one element is chosen from the input to become the result. Probability of choosing an element is proportional to its value.~\cite{ZEILER} 
	  \end{itemize}
	  
	  Pooling is defined not only by its type but also by its size and stride. The size of pooling defines how many values will be taken as an input - the bigger the size of pooling, the more information is accumulated in a single value. The pooling stride defines where will be the next submatrix with respect to its present location. Fig \ref{fig:max_pooling} illustrates this concept. 
	  
	  \begin{figure}[h!]
	    \centering
	    \includegraphics[scale=0.65]{max_pooling.png}
	    \caption{The result of applying the max pooling of size $2\times2$ with stride $1\times1$ to the input. Yellow square is the first pooling window, blue square is the second one. The values of the output are coloured accordingly.}
	    \label{fig:max_pooling}
	  \end{figure}
	  
	  Thanks to pooling the network is not affected by small changes of the patterns location. If only the pattern location is not in another window, the output of the network will not change at all. And even if the location of pattern has changed enough for it to be in another window, then the pooling will make this difference in distance smaller by being applied to submatrices not to single values.
  
	  When pooling is applied on the edges of the input same problems might be encountered as with the convolutions, namely some values will be underrepresented. It is worth noting that the output of the pooling is off smaller size than the input. To address these problems same techniques might be applied. 
	  	  
	\subsection{Summary}
	  In the convolutional subnetwork each layer applies three operations to the input, namely: convolution, which provides us with spatial invariance, activation function and pooling. As a result of this processing, the features are extracted from the data. These features are then used by the classifying subnetwork. 
	  
	  Convolution provides us with spatial invariance and is responsible for extracting features. Each layer of the convolutional part of the network has its own filters. These filters take advantage of the features that were already extracted in the previous layers. Therefore, the later in the network, the more complicated pattern the filter may recognise. It is worth noting that the patterns recognised by the filter are local - the filters are smaller than the input and find patterns that are also smaller than the input, which may appear few times in each sample. 
	  
	  Pooling provides us with resistance to small changes in patterns location.
  	  
      
      \section{Learning Algorithm}
      One might assume that, due to using convolution and pooling, the learning algorithm will also need to undergo a lot of changes. Luckily, it is enough to compute the partial derivatives for both these functions and include them in the learning algorithm which then will be ready to use.
      
      The learning algorithm proceeds the same way as any other learning algorithm for neural networks. At first the forward propagation is used to obtain the activation of the network for the data samples. Knowing the activation values it is possible to calculate the error in the activation function. This error is then used during backpropagation, when the updates to the network are calculated based on the error and the derivatives of the functions used for calculating the activation. The whole procedure is repeated until the performance of the network is satisfactory - the error in activation function in acceptably small.
      
       \section{Computation properties of convolutional neural networks}
      	  Convolutional neural networks have properties that make the computation fast, namely \emph{parameter sharing} and \emph{sparse interactions}~\cite{Bengio}. Both these properties are connected to using convolution. 
	  
	  Parameter sharing means that the same weight is used multiple times during calculating the activation for a sample.
	  Convolution matrices are much smaller than the input and are moved around it, so each time the same set of parameters is used to compute the output. Thanks to that, there is no need to have multiple sets of parameters that would discover the same feature in multiple places of the input - it is enough to have one filter that moves around that input. Because of that, the number of parameters is greatly reduced, and therefore the learning process speeds up. 
	  
	  In a typical neural network each element of input has influence on each element of the output which means that the interactions are dense. In convolutional neural networks the situation is different - the filters are much smaller than the input and discover local patterns, what leads to sparse interactions. This property again causes a reduction in the number of parameters in the model and makes the learning process much faster.
	
	
  \chapter{The Model}\label{chap:OUR_MODEL}
     In this chapter we will present the approach that have been used to tackle the problem of classification the provided data. First, the data itself will be described in detail. Afterwards, we will present both approach that we investigated during our experiments. The detailed description of the experiments will be given.
      
      \section{Goal}
      The goal of this work was to build a model that will well perform the task of classification of the provided data. To complete this task multiple obstacles had to be overcomed, i.e. small data size, missing labels, a big number of hyperparameters that had to be adjusted. 
      
      \section{Data}\label{sec:OUR_DATA}
	The dataset describes interactions between a single protein and multiple ligands. One might choose to see the dataset as a four dimensional matrix with axes dimensions described by: the number of ligands, length of the protein, 6 standard pharmacophore features of ligand and 9 types of interactions with amino acid\cite{2DSIFT}. One data sample can be seen as a 3-dimensional matrix that describes how a protein bounds with a specific ligand. The 3 dimensions are: the length of the protein (number of its residues), 6 standard pharmacophore features and 9 types of interactions with amino acid. A single data sample is presented on figure \ref{fig:single_data_sample2}. 
	
	\begin{figure}[h!]
	  \centering
	  \includegraphics{single_data_sample.png}
	  \caption{A single data sample.}
	  \label{fig:single_data_sample2}
	\end{figure} 
	
	The 6 pharmacophore features are: hydrogen bond acceptor, hydrogen bond donor, hydrophobic, negatively charged group, positively charged group, aromatic. The 9 types of interactions with amino acid are: any, with a backbone, interaction with sidechain, polar, hydrophobic, hydrogen bond acceptor, hydrogen bond donor, charged interaction, aromatic. 
	
	Even though it might be intuitive to look at this data as if it were 4-dimensional, it was stored in the memory in a 3-dimensional form by placing the 6 x 9 matrices adjacent to each other. If we had a protein with only three residues, name them A, B and C, a single sample would look like on figure \ref{fig:data_original}. 
	
	\begin{figure}[h!]
	  \centering
	  \includegraphics[scale=0.6]{original_dataset.png}
	  \caption{Data stored in the memory}
	  \label{fig:data_original}
	\end{figure} 
	
	The values constituting the dataset are discrete numbers in range 0 to 9. They describe how many interactions of a specific kind there is in a certain residue. The data was very sparse - more than 99\% of all values were zeros. 
	
	The dataset was stored in 3 files - each file contained samples of only one type: active, inactive, middle (not labled). Out of 5844 samples 2655 were labeled as active, 1945 was labeled as inactive and there were also 1244 unlabeled examples. 
	
	%2RH1_middle_2dfp.dat
	%    ||   0:   25795857.0   ||   1:   126528.0   ||   2:   58546.0  ||    3:   21098.0   ||   4:    6490.0   ||
	%    ||   5:       2957.0   ||   6:     1053.0   ||   7:     498.0  ||    8:     116.0   ||   9:     182.0   ||

	%    ||      0:      0.991640130587   ||      1:      0.00486396875447        ||      2:      0.00225061579018        ||    %    3:       0.000811045877449       ||      4:      0.00024948752226        ||      5:      0.000113672512068       ||    %    6:       4.04792543821e-05       ||      7:      1.9144034836e-05        ||      8:      4.45925309433e-06       ||    %    9:       6.99641433765e-06       ||
	    
        %2RH1_actives_2dfp.dat
        %   ||   0:    56394518.0    ||  1:   250661.0   ||   2:  125666.0  ||    3:   61630.0   ||   4:      14732.0 ||
        %   ||   5:        7482.0    ||  6:     3815.0   ||   7:    2893.0  ||    8:     733.0   ||   9:      534.0   ||
           
	%   ||       0:      0.991767075844  ||      1:      0.00440818249388        ||      2:      0.00220999142777        ||    %   3:       0.00108383947681        ||      4:      0.000259080369502       ||      5:      0.000131580187661       ||    %   6:       6.70914749967e-05       ||      7:      5.08769691128e-05       ||      8:      1.289070804e-05         ||      %   9:       9.39104787634e-06       ||
	    
	%2RH1_inactives_2dfp.dat
	%  ||    0:    41022606.0    ||  1:   204438.0   ||   2:   93580.0  ||     3:  34181.0   ||    4:       14486.0 ||
	%  ||    5:        3760.0    ||  6:     1494.0   ||   7:     592.0  ||     8:    284.0   ||    9:       166.0   ||

	%  ||      0:      0.991468858194  ||      1:      0.00494102959796        ||      2:      0.00226172017813        ||    
	%  3:       0.000826115167865      ||      4:      0.000350109836508       ||      5:      9.08748436608e-05       ||   
	%  6:       3.61082490503e-05      ||      7:      1.43079541083e-05       ||      8:      6.86395095736e-06       ||    
	%  9:       4.01202767226e-06       ||

	\subsection{Data preprocessing}\label{sec:data_processing}
	  In order to extract most promising features, the data has been preprocessed. Our goal was to enable the model building such patterns that could detect whether a bound of a specific kind was present in both adjacent residues. We expect that such approach might lead to discovering of interesting correlations. 
	  
	  To achieve such a form of the dataset that would enable this approach, the dataset was extended in the following way:
	  \begin{enumerate}
	   \item three copies of the dataset have been created.
	   \item Each copy was put just below the previous one. 
	   \item Each copy was shifted in such a way that going from top to bottom and from left to right would preserve the order of the residues. The shift forced us to either complement each row with zeros or to cut off the residues that would stick out.
	   \item We decided not to cut off any residues in order to have each of them the same number of times in the dataset - this way no residue will be underrepresented.
	  \end{enumerate}

	  As a result each sample had 18 instead of 6 rows and 18 columns more. 
	  
	  The schema of this approach is shown on figure \ref{fig:extended_data} which depicts the simple example of a protein with only three residues. It can be observed that a convolution window broader than 9 would be able to detect whether an interaction of some type is present in both adjacent residues while convolution window higher than 6 would be able to detect if two adjacent residues have same pharmacophore features. 
	  
	  \begin{figure}[h!]
	    \centering
	    \includegraphics[scale=0.50]{dataset_extension.png}
	    \caption{Data after preprocessing.}
	    \label{fig:extended_data}
	  \end{figure}
	  	
      \section{The Architecture}
      In this section we will describe the type of architecture which we used for experiments. All the models we have trained were convolutional neural networks with one or two convolutional rectified linear layers (those are layers that use as activation function the rectifier linear function, see \ref{sec:ACTIVATION_FUNCTIONS}). Each layer had 16 or 32 output channels that correspond to number of filters created in each layer. The number of layers have been chosen in such a way that the learning process will not take too much time and the data size will not be reduced too much. Rectifier activation function was used because of its eligible properties~\cite{DUTCH}.  
   
      The possible convolution window's sizes were chosen in a way that would enable the model to find filters catching correlation between same types of interaction in two adjacent residues, what was described in section \ref{sec:data_processing}. The convolution windows were of size $(width, height) \in \{6, 8, 10, 12\} \times \{4, 5, 6, 7, 8\}$.The convolution window's strides were of size $(width, height) \in \{2, 4, 6\} \times \{2, 3\}$. 
      
      If both convolution window size and stride had big values in the first layer, it could happen that the data size in the second layer would be too small to satisfy the conditions above. In such cases the convolution window's size and stride in the second layer were reduced in such a way that the convolution window's size would always be smaller than the data size. Moreover, the convolution window's stride would always be smaller than the convolution window and, if only it was possible (i.e. all dimensions of the convolution window were smaller then the corresponding dimensions of the data), small enough to enable existence of at least two ``windows'' in each dimension. 
      
      The shape of pooling windows was (1, 1), (2, 1) or (2, 2) and smaller by at least one than the data size in each dimension so moving the pooling window was always possible. Pooling stride was always equal to or smaller by half than the pooling window in each dimension. Max pooling was used. 
      
      The last layer of the network was a softmax layer with two neurons. It was used to classify the sample based on features extracted by the convolutional part of the network. 
      
	\subsection{Finding best architecture}
	Due to many hyperparameters, there exist many models that fulfill our architecture restrictions and therefore it is not possible to train and measure the performance of all possible architectures. To find the best one we used the tree of Parzen estimators algorithm (see appendix \ref{appendiks}) provided by a Python library - Hyperopt~\cite{HYPEROPT} and let it sample 20 models. 
		
	Each architecture that was tested was chosen by hyperopt module. Based on the performance of the already tested architectures hyperopt was choosing another one. Each architecture was passed from hyperopt to the function responsible for measuring the performance of the model. We will call this function an objective function. 
	
	 \begin{figure}[h!]
	  \centering
	  \includegraphics[scale=0.6]{control_flow.png}
	  \caption{The control flow of the experiments.}
	  \label{fig:control_flow}
	\end{figure} 
	
	\subsubsection{Objective function for hyperopt}
	Each time the objective function created five models of a given architecture and then trained and measured the performance of each. Cross validation procedure was used to obtain different training data for each model. Validation and test set included only labeled examples because classifying unlabeled examples would not be possible. All the unlabeled samples were added only to the training set. The cross validation proceeded as follows: the active and inactive samples were split into five parts of even size. One part became the validation set, one part the test set and the other three parts along with all the unlabeled examples became the training set. The whole procedure was repeated five times. 
	
	Each time after training the model its performance on testing data was measured and stored. At the end the mean value of scores of all five models was returned to the hyperopt module. The score used for measuring the performance of the model was receiver operating characteristic (ROC) with Youden's J statistic. We will cover this topic in details later in this section. Algorithm \ref{alg:cross_validation} shows pseudo code for the cross validation procedure. 
	
	\begin{algorithm}
	\caption{Cross validation}\label{alg:cross_validation}
	\begin{algorithmic}[1]
	\Procedure{objective\_func}{sample, data\_labeled, data\_unlabeled}
	\State
	\State $\textit{scores} \gets \textit{empty list}$
	\State
	\For {$\textit{k} \in [0...4]$} 
	  \State $\textit{train\_set, validation\_set, test\_set} \gets \textit{split(data\_labeled, k)} $
	  \State $\textit{train\_set} \gets \textit{train\_set + data\_unlabeled}$
	  \State $\textit{model} \gets \textit{build\_model(sample)}$
	  \State $\textit{model} \gets \textit{train(model, train\_set, validation\_set)}$
	  \State $\textit{score} \gets \textit{measure\_performance(model, test\_set)}$
	  \State $\textit{scores.append(score)}$
	\EndFor
	\State       
	\State
	\Return{$\textit{mean(scores)}$}
	\EndProcedure
	\end{algorithmic}
	\end{algorithm}
	
	\subsubsection{Training the model}
	The model provided by the hyperopt module was built five times and every time it was trained on another part of the data which was obtained using the cross validation procedure. After each epoch of training the optimal threshold was calculated on the validation set. All samples, for which activation value was above the threshold, were then classified as active samples and those, for which activation value was below the threshold, were classified as inactive. Keep in mind, that there were no unlabeled examples in the validation set.
	
	In order to compute the optimal threshold, we used the receiver operating characteristic (ROC) - a method that is widely used in such problems~\cite{ROC_1, ROC_EF} (see appendix \ref{appendiks} for more information about ROC). To measure the quality of the threshold the Youden's J statistic~\cite{YOUDEN} (see appendix \ref{appendiks} for more information) was used. Afterwards, the model's performance on validation data was measured. The score was the Youden's score. If the performance for this model was best until this point of time, the whole model (i.e. all its parameters along with the computed threshold) was dumped to hard drive for later reference. As a result, at the end of the learning process the model's best version was remembered and could be read in order to measure its performance on the testing set and append its score to the list in the cross validation procedure. The threshold calculated during the learning phase was used. The score to measure the performance of the model was the Youden's score. 
	
	
	The details of the learning algorithm are covered in section \ref{sec:learning_algorithm}.	
	
      
    \section{The Learning Algorithm}\label{sec:learning_algorithm} 
    The provided data included unlabeled examples. Two approaches that would enable using these examples to training the model were tested. 
    
      \subsection{Na\"{i}ve Approach}
      Training set was constructed in the following way: all examples were included in the training set two times - the labeled samples were included with their label and the unlabeled examples were included once with positive label, and once with negative label. This way the impact on classification of unlabeled data was minimised while the unalabeled data could have been used to improve parameters in the convolutional part of the model. 
	  
      \subsubsection{Example:}
      If there were the following samples: [A, B, C] along with the following labels: [act, inact, unlabeled] then the training set would look like this: [A, A, B, B, C, C] and the labels would be [act, act, inact, inact, act, inact]. 
      
    For this approach the stochastic gradient descent algorithm included in Pylearn2 Python library was used~\cite{Pylearn2}. 
	  
      \subsection{Fancy Approach}
      In this approach each sample was included in the dataset only once. In order to train the model, a variation of stochastic gradient descent algorithm was written. This enabled using unlabeled examples during the learning process. The SGD implementation provided in Pylearn2 was used as a base~\cite{Pylearn2}. Major changes were introduced in the training function in such a way that the unlabeled examples were used to adjust the parameters of the convolutional part of the model only and had no impact on the classification part. The pseudo-code of this algorithm can be found below as Algorithm 2. 
      
      \begin{algorithm}
      \caption{Learning}\label{euclid}
      \begin{algorithmic}[1]
      \Procedure{train}{sample, label}
      \If {$\textit{sample is unclassified}$}
	\State $\textit{parameters\_on\_enter} \gets \textit{current\_parameters}$
	\State
	\State $\textit{SGD(sample, inactive)}$
	\State $\textit{diff\_vec\_1} \gets \textit{current\_parameters} -\textit{parameters\_on\_enter}$
	\State $\textit{current\_parameters} \gets \textit{parameters\_on\_enter}$
	\State
	\State $\textit{SGD(sample, active)}$
	\State $\textit{diff\_vec\_2} \gets \textit{current\_parameters} - \textit{parameters\_on\_enter}$
	\State $\textit{current\_parameters} \gets \textit{parameters\_on\_enter}$
	\State
	\State $\textit{update\_vector} = \textit{new vector of length same to difference vectors}$
	\For {$\textit{el1, el2, up\_el} \in \textit{zip(diff\_vec\_1, diff\_vec\_2, update\_vec)}$}
	  \If {$\textit{sign(el1)} == \textit{sign(el2)}$}
	    \State $\textit{up\_el} \gets \textit{combination\_function(el1, el2)}$
	  \Else
	    \State $\textit{up\_el} \gets 0$
	  \EndIf
	\EndFor
	\State
	\For {$\textit{up\_el} \in \textit{update\_vec}$}
	  \If {$\textit{up\_el is responsible for updating the classification part}$}
	    \State $\textit{up\_el} \gets 0$
	  \EndIf
	\EndFor
	\State
	\State $\textit{current\_parameters} \gets \textit{parameters\_on\_enter} + \textit{update\_vector}$
	\Else
	\State $\textit{SGD(sample, label)}$
      \EndIf
      \State
      \EndProcedure
      \end{algorithmic}
      \end{algorithm}
      
      For labeled samples the learning process was performed with no changes. When the sample was unlabeled the network parameters were stored and then the sample was presented to the network as if it was labeled as inactive. During this process the network parameters were updated. The difference in the network parameters was stored and old parameters were restored. Afterwards, the sample was presented to the network again - this time as an active sample. The procedure was the same as before. After calculating the difference and restoring the old parameters the two vectors of differences were compared to produce the final vector of updates. 
           
           
      The final vector had the following properties:
      \begin{enumerate}
       \item the elements responsible for updating the classification part of the network were all zeros, therefore the unlabeled examples had only impact on learning parameter of the convolutional subnetwork and did not influence the classification part of the network. 
       \item the elements responsible for updating the convolutional part of the model were calculated in the following way:
	\begin{itemize}
	 \item if the corresponding elements of the two vectors had the opposite sign, then the corresponding element in the final vector was zero. As a result, the unlabeled samples were used by the network to learn only these filters that were useful for classifying samples of both classes
	  \item if the corresponding elements in both vectors had the same sign, then the corresponding element in the final vector was calculated using the values of the two elements. The final value could be:
	  \begin{itemize}
	    \item minimum by absolute value of the two elements
	    \item maximum by absolute value of the two elements
	    \item mean of the two elements
	    \item softmax mean of the two elements, i.e. having $x, y \in \mathbb{R}$ the softmax mean $\sigma$ is equal to $x \cdot \frac{e^x}{e^x + e^y} + y \cdot \frac{e^y}{e^x + e^y}$.
	    
	    \subsubsection{Remark}
	      It can be observed that $\frac{e^x}{e^x + e^y} \in [0, 1]$ for any $x$, $y \in \mathbb{R}$ and that $\frac{e^x}{e^x + e^y} + \frac{e^y}{e^x + e^y} = 1$, therefore $\sigma = x \cdot \frac{e^x}{e^x + e^y} + y \cdot \frac{e^y}{e^x + e^y}$ is a convex combination of $x$ and $y$, so $\sigma$ will be between $x$ and $y$. 
	  \end{itemize}
	\end{itemize}
      \end{enumerate}

      
      Please, see the example below to understand this concept more clearly.
            	
      Concluding, the update vector had zeros in part responsible for classification. If two corresponding values in the vectors of differences had opposite sign, then the corresponding value of the update vector was zero. All other elements were calculated using one of the combination functions. 
      
      \subsubsection{Example}
      Let $[+2, +5, +1, -3, +5, +7]$ and $[-2, +3, -1, -7, -7, +7]$ be the vectors of differences, elements 1 to 4 were responsible for updating the convolutional part, elements 5 and 6 were responsible for updating the classifying part of the network and the combination function used was minimum, then the final vector would be $[0, 3, 0, -3, 0, 0]$. Elements 5 and 6 are zeros because they are responsible for updating the classification part of the network. Elements 1 and 3 are zeros because the corresponding values in two vectors have opposite signs. Elements 2 and 4 are minimums by absolute value of the two corresponding values. This example is ilustrated in figure \ref{fig:combining}. 
      
      \begin{figure}[h!]
	  \centering
	  \includegraphics[scale=0.7]{combining_min.png}
	  \caption{Example of the min combining function}
	  \label{fig:combining}
	\end{figure} 
	
   \chapter{Results} % TODO napisać ten rozdział
    \section{not yet}
     
	
  \chapter{Discussion}
    \section{napisać co się udało}
  % TODO napisać też co się udało
    \section{Future work}
      There are still many ways to improve and further investigate the proposed method.
      
      All experiments should be conducted on a bigger set of data which is important for relevance of the results.
      
      The emphasis should be put on investigating how the combination functions affect the networks ability to learn. There is a need to further look for new possible combination functions.
      
      As the experiments have shown, there is a need to enlarge the set of architectures that are being explored - the number of possible convolutional layers should be increased.
      
      Further, it is interesting to test other methods of evaluation the model and explore new methods of finding the optimal threshold for classification. In particular, we would like to investigate the approach which would enable finding two thresholds. Note that the model that created the dataset also used two thresholds so the new approach will lead to a greater resemblance (see section \ref{sec:PROBLEM} for more information). For evaluation the model such metrics as enrichment factor~\cite{ROC_EF, EF_BEDROC} or BEDROC~\cite{EF_BEDROC} can be used.
      
      The two-threshold model will leave some samples unlabeled. It is of vital importance to explore what error functions can be used in such approach.

      Moreover, tt is important to investigate if representing the data in the memory in a 4-dimensional instead of 3-dimensional form can improve the performance of the model (see \ref{sec:OUR_DATA} for details). Using another form of representing the data in the memory might lead to discovering other patterns which can be more relevant for this data.
      
      During the experiments we realised that creating three copies of the data during preprocessing (see \ref{sec:OUR_DATA} for details) might be redundant. We want to inspect if having only two copies would cause a big difference in performance. Reducing the size of data will also lead to shorter time of learning pahse.
      
      It very important to note, that this work is only the first attempt to aprroach this problem. In reality, the goal is not only to classify molecules as active or inactive but also provide a ranking of those which are most likely to be active. This is because conducting wet-laboratory tests is very expensive and it is desirable to choose only few molecules to test but such that will likely give expected results. Testing all the molecules which are classified as active is not possible. In the future a model that provides not only classification but also ranking shall be proposed.
      
      Since the docking is a stochastic process that is not fully repetitive, it is a common procedure to dock the same molecule multiple times and each time calculate score for this docking. In the data provided each pair protein-molecule was provided only once with its class (active, inactive, middle). In the future it is recommended to use a dataset in which all docking trials are included with the score for each (not the class).
      
      Finally, there exist some techniques for deep and convolutional neural networks that might further improve the performance of our model, i.e dropout and dropconnect methods~\cite{DUTCH}.
      
      bedroc, enrichment factor, bo problem farmakologiczny jest inny

   \begin{appendices}
    \chapter{Dictionary}\label{appendiks}
      \emph{The tree of Parzen estimators algorithm} or \emph{the tree-structured Parzen estimator} is a method of finding the best hyperparameters of the network. The algorithm runs sequentially - a model is build and its performance is measured. Based on this knowledge another model is chosen and and the whole procedure is repeated.
      
      
      \emph{Receiver Operating Characteristic} (ROC) is a plot that measures ratio of false positive rate ($FPR$) to true positive rate ($TPR$). $FPR = \frac{FP}{FP + TN}$, where $FP$ is the number of false positive examples and $TN$ is the number of true negative examples. $TPR = \frac{TP}{TP + FN}$, where $TP$ is the number of true positive examples and $FN$ is the number of false negative examples. The example plot is presented on figure \ref{fig:ROC}.
      
      \begin{figure}[h!]
	  \centering
	  \includegraphics[scale=2]{Roccurves.png}
	  \caption{Example of four ROC curves. The dashed line is the ROC curve of a random classifier. The other curves are ROC curves of three binary classifiers\cite{bibROC}.}
	  \label{fig:ROC}
      \end{figure} 
      
      The ROC is used to measure the performance of a binary classifier. The bigger the value of true positive rate and the smaller the value of the false positive rate, the better the model. Therefore, a classifier that would lay in point ${(0, 1)}$ is the best possible model - one that classifies all labels correctly. For each model that returns score for a sample we might decide  what should be the threshold above which the model will classify samples as positive. All samples with the score below this threshold will be classified as negative. For a given model we might move this threshold along the axis and plot the performance of the model for each threshold. This way the ROC-curve will be created. There are several methods to score the points along the ROC curve in order to nominate the best threshold for a given model.
      
      \emph{Youden's J statistic} is one of the methods to score the points along the ROC curve. Mathematically it is described by the following equation:
      
      \begin{center}
      $J = sensitivity + specifity - 1$,
      \end{center}

      
      where
      \begin{center}
      $sensitivity = TPR = \frac{TP}{TP + FN}$
       
      $specifity = 1 - FPR = \frac{TN}{TN + FP}$.
      \end{center}
      
      The intuition is that the farer the point is from the line which depicts the ROC curve of a random classifier, the better is the model which generated that point. Note, that in this context 'model' means all model's parameters \emph{and} its threshold. Again, point $(0,1)$ is the one that is the farthest away from the ROC curve of a binary classifier. On figure \ref{fig:YOUDEN} a sample ROC curve with best classifier with respect to Youden's J statistic is shown.
      
      \begin{figure}[h!]
	  \centering
	  \includegraphics[scale=0.15]{ROC_Curve_Youden_J.png}
	  \caption{A ROC curve of a sample classifier. The best point according to the Youden's J statistic is shown with its distance to the ROC curve of a random classifier~\cite{bibYOUDEN}.}
	  \label{fig:YOUDEN}
      \end{figure} 
      
      
      
  \end{appendices}
      
  \begin{thebibliography}{99}
    \bibitem{DUTCH}
      J.~van Doorn,
      \emph{Analysis of Deep Convolutional Neural Network Architectures},
      21th Twente Student Conference on IT,
      (2014).
      
    \bibitem{GLOROT_BENGIO}
      Y.~Bengio, A.~Bordes, X.~Glorot,
      \emph{Deep Sparse Rectifier Neural Networks},
      International Conference on Artificial Intelligence and Statistics,
      (2011).
      
    \bibitem{YOUDEN}
     W.~J.~Youden,
     \emph{Index for rating diagnostic tests},
     Cancer, 3.1, 32--35,
     (1950).
     
    \bibitem{HYPEROPT}
     J.~Bergstra, D.~Yamins, D.~D.~Cox,
     \emph{Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms},
     Proceedings of the 12th Python in Science Conference, 13--20,
     (2013).
      
    \bibitem{ZEILER}
      M.~D.~Zeiler, R.~Fergus,
      \emph{Stochastic pooling for regularization of deep convolutional neural networks},
      arXiv preprint arXiv:1301.3557,
      (2013).
      
     \bibitem{Bengio}
      Y.~Bengio, I.~Goodfellow, A.~Courville,
      \textit{Deep Learning},
      MIT (w przygotowaniu),
      \url{www.iro.umontreal.ca/~bengioy/dlbook}.

      \bibitem{2DSIFT}
      S.~Mordalski, I.~T.~Podolak, A.~J.~Bojarski,
      \textit{2D SIFt - a matrix of ligand-receptor interactions},
      (w przygotowaniu).

      \bibitem{Breda2008}
      A.~Breda, L.~A.~Basso, D.~S.~Santos, W.~F.~de~Azevado~Jr., 
      \textit{Virtual screening for drugs: score functions, docking, and drug design},
      Current Computer-Aided Drug Design, 4, 265--272,
      (2008).

      \bibitem{Varnek2012}
      A.~Varnek, I.~Baskin,
      \textit{Machine learning methods in property prediction in cheminformatics: Quo Vadis?}
      J. of Chemical Information and Modelling, 52, 1413--1437,
      (2012).

      \bibitem{Geppert2010}
      H.~Geppert, M.~Vogt, J.~Bajorath,
      \textit{Current trends in ligand-based virtual screening: molecular representations, data mining methods, new application areas, and performance evaluation},
      J. of Chemical Information Modelling, 50, 205--216,
      (2010).

      \bibitem{Pethukov2006}
      P.~Pethukov, M.~Brunsteiner, R.~Uddin,
      \textit{Panthothenate synthetase: new inhibitors for tuberculosis target from a hierarchical virtual screening campaign},
      ACS'2006.

      \bibitem{Mordalski2011}
      S.~Mordalski, T.~Kosciołek, K.~Kristensen. I.~Sylte, A.~J.~Bojarski,
      \textit{Protein binding site analysis by means of structural interaction fingerprint patterns},
      Bioorganic \& Medicinal Chemistry Letters, 21, 6816--6819,
      (2011).

      \bibitem{Singh2006}
      J.~Singh, Z.~Deng, G.~Narale, C.~Chuaqui,
      \textit{Structural Interaction Fingerprints: a new approach to organizing, mining, analyzing, and designing protein-small molecule complexes},
      Chemical Biology Drug Design, 67, 5--12,
      (2006).
      
      \bibitem{ImageNet}
      A.~Krizhevsky, I.~Sutskever, G.~E.~Hinton
      \textit{ImageNet Classification with Deep Convolutional Neural Networks},
      Advances in Neural Information Processing Systems 25,
      (2012).

      \bibitem{CONV_NLP}
      R.~Collobert, J.~Weston
      \textit{A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
      Proceedings of the 25th International Conference on Machine Learning,
      (2008).
      
      \bibitem{GLOSSARY}
      C.~G.~Wermuth, C.~R.~Ganellin, P.~Lindberg, L.~A.~Mitscher,
      \textit{Glossary of terms used in medicinal chemistry},
      Pure and Applied Chemistry, Volume 70, Issue 5, 1129-–1143,
      (1998).
      
      \bibitem{CONV_NET}
      \textit{website of Parallel Architecture Research Eindhoven},
      \url{http://parse.ele.tue.nl/education/cluster2}

      \bibitem{docked}
      \textit{Ligand (biochemistry) - Wikipedia article},
      \url{https://en.wikipedia.org/wiki/Ligand_%28biochemistry%29}
     
      \bibitem{con_neur_INT}
      \textit{Deep Learning - Convolutional Neural Networks (LeNet)},
      \url{http://deeplearning.net/tutorial/lenet.html}

      
      \bibitem{Pylearn2}
      - IJ.~Goodfellow, D.~Warde-Farley, P.~Lamblin, V.~Dumoulin, M.~Mirza, R.~Pascanu, J.~Bergstra, F.~Bastien, Y.~Bengio,
      \textit{"Pylearn2: a machine learning research library"},
     arXiv preprint arXiv:1308.4214,
     (2013).

     \bibitem{bibROC}
     \textit{Receiver operating characteristic - Wikipedia article},
     \url{https://en.wikipedia.org/wiki/Receiver_operating_characteristic}
     
     \bibitem{bibYOUDEN}
     \textit{Youden's J Statistic - Wikipedia article},
     \url{https://en.wikipedia.org/wiki/Youden%27s_J_statistic}
     
     \bibitem{kernel_biology}
     B.~Sch{\"o}lkopf, K.~Tsuda, J.~P.~Vert,
     \textit{Kernel Methods in Computational Biology},
     Cambridge, MA: MIT Press,
     (2004).

     \bibitem{evaluation2007}
     B.~Chen, R.~F.~Harrison, G.~Papadatos, P.~Willett, D.~J~ Wood, X.~Q.~Lewell, P.~Greenidge, N.~Stiefl,
     \textit{Evaluation of machine-learning methods for ligand-based virtual screening},
     Journal of Computer-Aided Molecular Design, Volume 21, Issue 1, 53--62
     (2007).
     
     \bibitem{SVM}
     R.~N.~Jorissen, M.~K.~Gilson,
     \textit{Virtual Screening of Molecular Databases Using a Support Vector Machine},
     Journal of chemical information and modeling, Volume 45, Issue 3, 549--561,
     (2005).
     
     \bibitem{BKD}
     B.~Chen, R.~F.~Harrison, K.~Pasupa, P.~Willett, D.~J.~Wilton, D.~J~ Wood, X.~Q.~Lewell,
     \textit{Virtual Screening Using Binary Kernel Discrimination: Effect of Noisy Training Data and the Optimization of Performance},
     Journal of chemical information and modeling, Volume 46, Issue 2, 478--486,
     (2006).
     
     \bibitem{MLforINSilicio}
     J.-P.~Vert, L.~Jacob,
     \textit{Machine Learning for In Silico Virtual Screening and Chemical Genomics: New Strategies},
     Combinatorial Chemistry \& High Throughput Screening, Volume 11, Issue 8, 677-–685,
     (2008).
     
     \bibitem{NN_VS}
     L.~ Moln\'{a}r, G.~M.~Keser\H{u},
     \textit{A Neural Network Based Virtual Screening of Cytochrome P450 3A4 Inhibitors},
     Bioorganic \& Medicinal Chemistry Letters, 12, 419--421,
     (2002).
     
    \bibitem{ROC_1}
    T.~Unterthiner, A.~Mayr, G.~Klambauer, M.~Steijaert, J.~Wegner, H.~Ceulemans, S.~Hochreiter,
    \textit{Deep Learning as an Opportunity in Virtual Screening},
    Deep Learning and Representation Learning Workshop, NIPS,
    (2014).
    
    \bibitem{ROC_EF}
    H.~Geppert, M.~Vogt, J.~Bajorath,
    \textit{Current Trends in Ligand-Based Virtual Screening: Molecular Representations, Data Mining Methods, New Application Areas, and Performance Evaluation},
    Journal of chemical information and modeling, Volume 50, Issue 2, 205--2016,
    (2010).
    
    \bibitem{EF_BEDROC}
    V.~Venkatraman, P.~R.~Chakravarthy, D.~Kihara
    \textit{Application of 3D Zernike descriptors to shape-based ligand similarity searching}
    Journal of Cheminforminformatics, Volume 1, Issue 19, 
    (2009)
    
     
      %TODO: numpy, scipy?, sklearn metrics for f12 score, latex, draw.io
    
  \end{thebibliography}
  
    
\end{document}          



