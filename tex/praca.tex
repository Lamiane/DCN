%ANG: jak się wstawia przecinki w zdaniach, synonimy do usually
%ANG: kiedy przed ed podwaja się spółgłoskę?
%ANG: synonyms do ``as stated above'' ?
%TODO: liczba enterów wszędzie.

\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
%\usepackage{polski} %FIXME: może na końcu wywalić, chyba, ze ktoś z przypisów ma polskie nazwisko, o te nazwiska wcześniej zapytać
\usepackage[T1]{fontenc}
%\usepackage[sorting=nyt,style=apa]{biblatex}


% Title Page
\title{Praca Magisterska}
\author{Agnieszka Pocha}


\begin{document}
  \maketitle
  \begin{abstract}
    The goal of this work is to... ...drug design... This is achieved by applying (deep?) convolutional neural networks to the problem.
  \end{abstract}
  \tableofcontents
  
  %FIXME: napisac na samym początku co będzie po kolei, 
  
  \chapter{Introduction} %TODO: napisać ten rozdział
    \section{related work, że to ma oparcie w czymś i o czym jest praca}
  
  \chapter{The Problem and the Data}
    %FIXME {cytowania! jak to się robi? poczatek, koniec?}
    %FIXME co z wcięciami?
    
    \section{Terminology} %TODO: zmienić hasła na zdania   
    One of major areas of study nowadays is \textbf{drug design}. It is a quickly developing field, facing challenging problems, such as conducting experiments which are very expensive and extremely time consuming. Therefore, computer modelling is of vital importance. Artificial intellingence and machine learning have been successfuly incorporated to this field. (citation needed?). One of the most common problems in drug design is telling whether \textbf{protein} and \textbf{ligand} will together produce an \textbf{active} or \textbf{inactive} compund.\\
    
    \textbf{Proteins} are molecules built from \textbf{amino acid residues} forming a single \textbf{chain}. The number of residues defines the length of the chain. Proteins are the most elementary (łeż) things building bodies of living creatures, crucial for many major things going on in our bodies.\\
    
    ligand, protein-ligand docking, receptor, donor, active, not active, pharmacophore, interactions, active non/inactive protein\\ %TODO: dokończyć
    
    \textbf{big data} but not complete and from different sources%TODO: dokończyć
         
    \section{Fingerprints} %TODO: zmienić hasła na zdania, citations! czy to co ja używam to w ogóle fingerprinty są?
    One of the first questions that have to be answered before *any modelling task can be started* is: how will I represent my data? Some proteins have less than 100 residues while others might have even few thousands of them. The order of amino acids in the chain and their spatial arrangement carry a lot of information. It might seem that a natural representation of a protein will be a graph containing extra information about how the nodes are arranged in space. Unfortunately, graph algorithms are computationally expensive and it is nowadays not possible to use them (na czym się opieram?). Therefore, *we* need another representation, that will carry as much information as possible and *be computationally effective*. *For this reason* many types of fingerprints have been designed and they meet the criterions metioned.\\
    
    Conventionally, fingerprints represent a *protein/molecule* as a binary(?) vector. Each element of this vector *tells* whether a specific structure is present in the *protein/molecule* or not, e.g. whether the *protein/molecule* has a .......... (oprzeć się na jakiejś publikacji). Vectors are widely used in machine learning as data representation as they can be *compilled* into matrices which allows to *easy computation, easy proofs, happy algebra* <<We need a simpler representation - such on which we can quickly apply many function, such that is well designed for computers, for modern algorithms>> Even though representing a protein as a vector means loosing a lot of information about it, it enables effective computation and still provides us with reasonable results.\\
    
    As already said, there are many different fingerprints designed. They vary in length and the features included. Some of them are designed for specific tasks. (citation needed). In this work 2D-SIFt will be used.
  
    \section{Problem} %TODO: zmienić hasła na zdania
    drug design, innovative data representation from \cite{2DSIFT}, more details, what exactly am I trying to achieve? *Deep* Convolutional neural networks will be applied to the problem.
    
    \section{Datasets}
    The data consists of multiple datasets, each describing reactions between a single protein and multiple ligands. Each dataset consists of four dimensions described by: the number of ligands, length of the protein, 6 standard pharmacophore features of ligand and 9 types of interactions with amino acid\cite{2DSIFT}. One data sample can be seen as a 3-dimensional matrix describing how a single ligand bounds with a specific protein. The 3 dimensions are: the length of the protein (number of its residues), 6 standard pharmacophore features and 9 types of interactions with amino acid. Moreover each data sample is labeled by *?* (*(the *protein* is)* active), *?* (not active), *?* (no information). \\

    \{obrazek\} \\ %TODO
    
    The 6 pharmacophore features are: hydrogen bond acceptor, hydrogen bond donor, hydrophobic, negatively charged group, positively charged group, aromatic. The 9 types of interactions with amino acid are: any, with a backbone, interaction with sidechain, polar, hydrophobic, hydrogen bond acceptor, hydrogen bond donor, charged interaction, aromatic. \\
    
    %TODO:Values: 0, 1, 2 and their meaning !!! nie ma tego w publikacji!
    The values constituting the dataset are discrete, namely: 0, 1 and 2. 0 means there is no interaction of specific kind. 1 and 2? As stated above the labels are reperesented as ??? (not active), ??? (active), ?? (no information).
    
    \section{Sparsity} %TODO: zmienić hasła na zdania
    check if the data is sparse, it yes then state that it is and explain why
    
    \section{Data representation} %TODO: pewnie chcę napisać coś więcej + zmienić notatki na zdania
    Each data sample is represented as a vector of $r*6*9$ length, where $r$ is the length of the protein. Data samples constitute a dataset. Each dataset describes a reaction/bonding between a certain protein and the ligand.\\
    
    why was this particular fingerprint representation chosen %TODO: napisać
    
    
  \chapter{The Model or Deep Convolutional Neural Networks}
    
    \section{Deep Neural Networks}
      DNN

    \section{Convolutional Neural Networks}
      %TODO: opis w jednym zdaniu, czym to jest i do jakich danych (specyfika,nie konkrety) można je aplikować, dlaczego zwane konwolucyjnymi
      The simplest definition of convolutional neural networks is probably: neural networks that take adventage of using the convolution operation. Usually the CNN is *conceptually* divided into two subnetworks: first subnetwork is *built from* convolutional layers and is responsible for feature extraction, the second one is a classical neural network, e.g. multilayer perceptron. Its aim is to *poprawnie* classify the examples taking as input the features extracted by the previous subnetwork.\\
      
      \{obrazek\} \\ % jakiś typowy lenet
      
      In this section I will give motivation that stands behind using convolutional neural networks, *explain/define* what is the convolution operation, and give a detailed explanation of CNNs and its properties. Finally, I will describe what problems might arise while learning a CNN model and how to prevent them. The learning algorithm for CNNs will be given.
      
      \subsection{Motivation} %TODO: zmienić poniższe notatki w tekst 
	%TODO: why are CNNs so useful, awesome and important, w czym dokonały przełomu? (citation needed? praca hoelnderska?)
	Convolutional neural networks are *mostly* applied to image recognition, video analysis and natural language processing problems. %FIXME: citation needed?
	This attempts *are often succesful/often give better results than (any other) models*. %TODO: citation needed
	
      \subsection{Convolution} %TODO: zmienić poniższe notatki w tekst 
	Convolution operation takes as *operands* two functions (dziedzina? zbiór wartości?) and return a new function (dziedzina? zbiór wartości?) as a result. Mathematically, convolution is defined as: \\
	
	$c(t) = \int\limits_{-\infty}^\infty f(x)g(t-x)dx$ \\
	
	In the equation above we can see $c$ - a function returned by convolution operation, that is evaluated in point $t$. $c$ is defined as an integral over two other functions. $f$ is often called an input, while $g$ is often reffered to as a kernel.%FIXME: kernel i input nie zamienione miejscami? definicja dobra? citation needed?
	  \\
	
	It is often useful to see kernels as feature extractors. %TODO
	why kernels can be seen as feature extractors?
	
      \subsection{Computation Flow} %TODO zmienić poniższe notatki w tekst
	As stated above, CNNs can be conceptually divided into two subnetworks. In this subsection I will describe how the signal is processed within the convolutional subnetwork. I will not *dig into* the classifying subnetwork as any neural network that can be used to classify objects might be used. Many such networks exist*s* and they are well described in the literature. \\ %TODO: citation needed?
	
	In each layer of the convolutional subnetwork there are three *elemental* operations *wykonywane*. Firstly, the input is convoluted with a kernel matrix. The result of this operation is an input to the activation function. If the input is a matrix *(and it usually/always is a matrix)* each element forms a single input to the activation function *(a może da się inaczej?)*. Finally, the pooling is applied to the result. \\
	
	\{obrazek\}  może też wzorek? pooling(sigmoid(convolution(input))) \\
	
	One might also imagine three consecutive seperate layers: a convolutional layer, a *classical* layer that applies activation function *(an activation layer)* and finally, a pooling layer.\\
	
	In the following subsections I will describe in details how each of this operation exactly works. *Szczególna uwaga zostanie zwrócona na to, jak zmieniają się wymiary danych na każdym etapie*\\
	
	\{obrazek\} chyba ten u góry tylko
	
	\subsubsection{Convolution for neural networks}%TODO: zmienić poniższe notatki w tekst 
	  The convolution operation was defined as: \\
	  
	  $c(t) = \int\limits_{-\infty}^\infty f(x)g(t-x)dx$ \\
	  
	  It is worth considering how this equation can be applied to neural networks. Real values cannot *?* be represented in *computer calculations* therefore using an integral is *not possible*. It is desirable to have the equation (*number*) in discrete form.\\
	  
	  $\overline{c}(t) = \sum\limits_{x = -\infty}^\infty \overline{f}(x)\overline{g}(t-x)$ \\%FIXME: bengio i wikipedia używają [], dlaczego? czy ja też powinnam? 
	  
	  *From now on* *we/I* will call $f$ an input, $g$ a kernel and $c$ na output.\\
	  
	  Usually, the kernel that is used is much smaller then the input and convolution is appplied multiple times. Each time kernel is convoluted with a submatrix of input. The result of this operation is a scalar, and a result of a whole process is a matrix.\\
	  
	  As stated above, each kernel might be seen as a filter extracting a single feature. Usually, *we* need to extract multiple features from the image. As a result, in each layer many convolutional kernels are used. It is worth noting, that kernels are different in each layer. Kernels in the next layer work on the features extracted from the previous layer. *It might be shown* (citation needed), that the features from the next layer are constructed of features from the previous layer. Therefore, in each layer features are more and more complicated.\\
	  
	  *Let's* assume that the input is a $I1$x$I2$ matrix and the kernel is $K1$x$K2$ matrix *with* $1$x$1$ stride. Therefore the first submatrix to convolute with a kernel will be *[1:$K1$]$x$[1:$K2$]* and it will return a *single value/scalar*. The last sub,atrix will be *[$I1-K1+1$:$I1$]x[$I2_k2+1$:$I2$]* and it will produce a  *single value/scalar* as well. As a result the output will be a ($I1-K1+1$)x($I2-K2+1$) matrix.\\
	  
	  \{obrazek\} ilustrujący problem z kernelem i brzegami. Niech na nim będzie, first, second, (może third) i last submatrix\\
	  
	  It is *easy* to observe that the values laying close to the edge of the input matrix will be underrepresented and consequently the output matrix will be of smaller size than the input matrix. There is a variety of ways to address this problem, e.g. zero-pad.\\
	  
	  %FIXME: cytaty do Bengia, sprawdzić nazwy i szczegóły, cytaty do matlaba?
	  
	  Opisać jakie rozwiązanie zostało użyte przez nas. %TODO:
	
	\subsubsection{Activation function} %TODO: zmienić poniższe notatki w tekst 
	  różne możliwe typy
	
	\subsubsection{Pooling} %TODO: zmienić poniższe notatki w tekst 
	  Pooling is an operation that takes as an input multiple values and returns *the statistic(s) of these values*. It is usually applied on a matrix. It takes as *an* input the submatrices and returns a single value as a result. 3 most common *(citation needed?)* types of pooling are:
	  \begin{itemize}
	   \item max pooling - the max value is returned
	   \item average pooling - the average value is returned
	   \item weighted average pooling - the weighted average is returned. Weights are usually *(citation needed?)* defined by the distance from *what?*
	  \end{itemize}
	  
	  types: Bengio, 181, pierwszy pełny akapit, pooling shape and stride $\rightarrow$ boost computational efficiency. \\

	  \{obrazek\} jak wygląda max pooling \\
	  
	  Pooling is defined not only by its type but also by its size and stride. The size of pooling defines how many values will be taken as an input - the bigger the size of pooling, the more information is *lost*. The pooling stride defines where will be the next submatrix with respect to the pravious one. Fig *???* illustrates this concept. \\
	  
	  \{obrazek\} pooling size and stride \\
	  
	  The same problems might be encountered with pooling on the edges as with the convolutions, namely the output of the layer will be off smaller size than the input unless some techniques avoiding it will *are/will be* applied.
	  
	   The kernel is smaller than the input. It is moved around the picture. It will find feature everywhere - we need this własność spatial invariance. %FIXME: ogarnąć ten akapit
	  
	  \{obrazek\} dlaczego daje nam invariance
	
	\subsubsection{Summary} %TODO: zmienić poniższe notatki w tekst 
	  During the convolutional subnetwork flow the features are extracted from the data. These features are later used by the *classifying* subnetwork. *Convolution/pooling* provide us *with* spatial invariance and *this other awesome thing*. In the convolutional subnetwork each layer applies three operations to the input, namely: convolution, activation function and pooling. \\ %FIXME: JĘZYK!
	  
	  $output = pooling(activation\_function(convolution(input, kernel)))$\\ %FIXME: notacja (ten sam wzorek jest w computation flow)
	  
	  Convolution and pooling might decrease the size of the input. This might be avoided by zero-pad or other methods. Several types of pooling and activation functions have been provided. %FIXME: JĘZYK!
	  
      \subsection{implementationally awesome things} %FIXME: change this stupid title
      
	\subsubsection{Fast computation}
	  (spatial invariance) $\rightarrow$ temu nie musimy też mieć osobnych macierzy na feature w każdym miejscu - oszczędzamy pamięć na parametry (i efektywnośc, bo im więcej paramterów, tym wolniej się uczymy). Small kernel $=$ litlle parameters.
	
	\subsubsection{sparse interactions} %TODO: zmienić poniższe notatki w tekst 
	  because kernel is smaller then data so it not kazdy z kazdym but some with some (sparse) $\rightarrow$ computational boost, kernel is small and moved around input - less parameters, instead of a big matrix we store a small one that runs over the data\\
	  
	  \{obrazek\} %TODO
	  
	\subsubsection{parameter sharing} %TODO: zmienić poniższe notatki w tekst 
	  Connected to the fact thet we move the convolution kernel around \\
	  
	  \{obrazek\} a moze nie?
	  
	\subsubsection{equivariant representations} %TODO: zmienić poniższe notatki w tekst
	  equivariance - property of *what?* meaning that if the input changes that output changes the same way. $f(g(x)) = g(f(x))$ Intuition about it: detecting feature in a particuler place - feature elsewhere - we find it elsewhere. To what types of transformation is convolution equivariant and to which transformations it isn`t?
      
      \subsection{Learning Algorithm} %TODO: backpropagation, state that this one is best (citation needed!)
	A *zmieniona wersja* of backpropagation algorithm has been provided to *include the changes that must be applied because of* the convolution operation and avoid the diminishing gradient flow. In this subsection the *zmienona wersja* of backpropagation is given and the problem of diminishing gradient flow is *addressed*.
	
	\subsubsection{The problems with a classical backpropagation} %TODO: notatki w tekst
	  diminishing gradien flow, niedouczanie się, przeuczanie się, obczaić co o tym mówił Larochelle, on to chyba jednak mówił o głębokich. Wtedy to i tak napisać i przerzucić do głębokich.
	
	\subsubsection{Diminishing gradient flow} %TODO: zmienić poniższe notatki w tekst, sprawdzić czy w CNNach tez przypatkiem nie ma tego probloemu, bo w głębokich na pewno jest
	  co to jest, skąd się bierze, można się wesprzeć wykładami Larochelle, on poleca dużo paperów zawsze.
	  
	\subsubsection{Backpropagation} %TODO: zmienić poniższe notatki w tekst
	  See (Goodfellow, 2010) from Bengio
	
      \subsection{Extensions} %FIXME this title shall not last! %TODO: zmienić poniższe notatki w tekst
	dropout/dropconnect method, activation functions for dropout, other things from the Dutch paper
	
	
    \section{Why was this model chosen}
	... Having said that kernels might be used as feature extractors it's worth considering what kinds of features might be discovered in the provided data. ...
	
  \chapter{What can be improved} %TODO: napisać ten rozdział
    \section{Everything...}
      ...can be improved.\\

  \begin{thebibliography}{99}
    \bibitem{DEEP}
      Yoshua Bengio, Ian J. Goodfellow, Aaron Courville - \emph{Deep Learning}
    \bibitem{2DSIFT} %FIXME jak się robi przypisy? w sensie formalnym, nie latexowym
      Stefan Mordalski, Igor Podolak, Andrzej J. Bojarski - \emph{2D SIFt - a matrix of ligand-receptor interactions}
    %TODO: pylearn2, Fingerprints, Bengio/... Deep NNs?, cool staff from Ph.D
    
  \end{thebibliography}
  
  \chapter{Irrelevant} %TODO: wywalić na końcu
    \section{zero-pad methods in detail}
      The easiest way is to let these values stay underrepresented (in MATLAB *citation?* this methodology is called valid), another one is to enlarge the input matrix by adding zeros *at the edges* - this is called zero-pad. One can either add enough zeros for each element of the original matrix to be convolutet exactly the same number of times (in MATLAB *citation?* this methodology is called full) or take only enough zeros for the output matrix to have the same size as the input matrix (in MATLAB *citation?* this methodology is called same).\\
	  
      \{obrazek\} ilustrujący te przykłady \\
	  
      One can question *legitimacy* of such approach. Adding zeros invites new information into the matrix and might cause additional noise. Instead of adding zeros one might try to change a matrix into torus or instead of zeros use the values that already are present in the original matrix. The added values might be symmetrical *lustrzane odbicie.*
	  
      \{obrazek\} ilustrujący te przykłady 
      
    \section{pooling}	  
      is pooling subsampling and if yes then why is polling subsampling? 

    
\end{document}          



