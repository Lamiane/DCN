!obj:pylearn2.train.Train { # our experiment
    # Here we specify the dataset to train on. We train on only the first 25k of the examples, so
    # that the rest may be used as a validation set.
    # The "&train" syntax lets us refer back to this object as "*train" elsewhere in the yaml file
    dataset: &train !obj:pylearn2.scripts.icml_2013_wrepl.emotions.emotions_dataset.EmotionsDataset {
        which_set: 'train',  # 'train' or 'test'
        start: 0, # starting index
        stop: 25000, # last index 25K of examples
        # We preprocess the data with global contrast normalization
        preprocessor: &prepro !obj:pylearn2.datasets.preprocessing.GlobalContrastNormalization {
            sqrt_bias: 10, # there is no documentation on preprocessing TODO: create it
            use_std: 1
            }
    },
    # Here we specify the model to train as being an MLP
    model: !obj:pylearn2.models.mlp.MLP {
        layers : [ # here ve specifiy the layers, there will be two
            # To make this baseline simple to run, we use a very small and cheap convolutional
            # network. It only has one hidden layer, consisting of rectifier units with spatial
            # max pooling.
            !obj:pylearn2.models.mlp.ConvRectifiedLinear { # no documentation!
                layer_name: 'h0', # layer name, can be anything, shall be unique
                kernel_shape: [8, 8], # shape of convolutional matrix
                pool_shape: [4, 4], # shape of pooling matrix
                pool_stride: [2, 2], # :( TODO: find out
                output_channels: 32, # TODO: what is it?
                irange: .05, # TODO: find out
                # Rather than using weight decay, we constrain the norms of the convolution kernels
                # to be at most this value # TODO so convolution kernels can change values during learning?
                max_kernel_norm: .9
            },

            !obj:pylearn2.models.mlp.Softmax { # output layer, will be softmax
                layer_name: 'y', # layer name, can be anything, shall be unique
                # The classes are unbalanced. Set the bias parameters of the softmax regression
                # to make the model start with the right marginals. This should speed convergence
                # of the training algorithm. TODO what?!
                init_bias_target_marginals: *train, # TODO work this out
                irange: .0,
                # There are seven different emotions to learn to recognize, i.e., 7 class labels
                n_classes: 7 # number of classes
            }
        ], # / end of layers
        # The inputs are 48x48 pixel images
        input_space: !obj:pylearn2.space.Conv2DSpace {
            shape: [48, 48],
            num_channels: 1
        }
    },
    # We train using SGD and momentum
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        batch_size: 100, # just a batch size
        learning_rate: .001, # just a learning rate
        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum { # we will use momentum
            init_momentum: .5, # init momentum value
        },
        # We monitor how well we're doing during training on a validation set
        # If not specified, no monitoring is used. If specified to be a Dataset, monitor on that Dataset.
        # If specified to be dictionary, the keys should be string names of datasets, and the values should be Datasets.
        monitoring_dataset: # no documentation TODO: work this out
            {
                'valid' : !obj:pylearn2.scripts.icml_2013_wrepl.emotions.emotions_dataset.EmotionsDataset {
                    which_set: 'train', # 'train' or 'test'
                    start: 25000, # start index
                    stop: 28709, # last indes
                    preprocessor: *prepro # pointer to the preprocessing rule
                }
            },
        # We stop when validation set classification error hasn't decreased for 10 epochs
        termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {
            channel_name: "valid_y_misclass",
            prop_decrease: 0.,
            N: 10
        },
    },
    # We save the model whenever we improve on the validation set classification error
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             channel_name: 'valid_y_misclass',
             save_path: "${PYLEARN2_TRAIN_FILE_FULL_STEM}_best.pkl" # path where the model will be stored
        },
    ],
}
