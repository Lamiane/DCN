!obj:pylearn2.train.Train { # our experiment
    dataset: &train !obj:pylearn2.testing.datasets.random_one_hot_topological_dense_design_matrix { # some dataset
        rng: !obj:numpy.random.RandomState { seed: [2013, 3, 16] }, # TODO
        shape: &input_shape [10, 10],   # input shape
        channels: 1,    # number of channels in dataset
        axes: ['c', 0, 1, 'b'], # TODO
        num_examples: 12,       # TODO
        num_classes: 10         # number of classes in the dataset
    },
    model: !obj:pylearn2.models.mlp.MLP { # our model will be MLP
        batch_size: 2,  # just a batch size
        layers: [ # defining layers
                 !obj:pylearn2.models.mlp.ConvElemwise { # convolutional layer, elementwise
                     layer_name: 'h0',      # layer name, can be anything, shall be unique
                     output_channels: 8,    # number of output channels
                     pool_type: "max",      # type of pooling
                     kernel_shape: [2, 2],  # shape of convolutional matrix
                     pool_shape: [2, 2],    # shape of pooling matrix
                     pool_stride: [2, 2],   # TODO
                     irange: .005,          # TODO
                     # Rather than using weight decay, we constrain the norms of the convolution kernels
                     # to be at most this value # TODO so convolution kernels can change values during learning?
                     max_kernel_norm: .9,
                     nonlinearity: !obj:pylearn2.models.mlp.SigmoidConvNonlinearity {} # we will use sigmoid function
                 },
                 !obj:pylearn2.models.mlp.Softmax { # the second (output) layer will be Softmax
                     max_col_norm: 1.9365,  # TODO
                     layer_name: 'y',   # layer name, can be anything, shall be unique
                     n_classes: 10,     # number of classes
                     irange: .005       # TODO
                 }
                ],
        input_space: !obj:pylearn2.space.Conv2DSpace {  # parameters of input, it'll be Conv2D - 4D tensor
            shape: *input_shape,    # shape of input space
            num_channels: 1,        # number of channels
            axes: ['c', 0, 1, 'b'], # TODO
        },
    },
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {   # will use SGD as learning algorithm
        learning_rate: .05, # just a learning rate
        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum { # we'll use momentum
            init_momentum: 0.5, # initial momentum value
        },
        # We monitor how well we're doing during training on a validation set
        # If not specified, no monitoring is used. If specified to be a Dataset, monitor on that Dataset.
        # If specified to be dictionary, the keys should be string names of datasets, and the values should be Datasets.
        monitoring_dataset:
            {
                'train': *train # TODO: how exactly does it work?
            },
        cost: !obj:pylearn2.costs.mlp.dropout.Dropout { # we will use dropout! TODO: why is this cost?
            input_include_probs: { 'h0' : .8 }, # TODO
            input_scales: { 'h0': 1. }          # TODO
        },
        termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter {  # when will we stop?
            max_epochs: 3   # after 3 epochs
        },
        update_callbacks: !obj:pylearn2.training_algorithms.sgd.ExponentialDecay { # TODO: ???
            decay_factor: 1.00004,
            min_lr: .000001
        }
    },
    extensions: [ # cool stff
        !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor { # we will adjust momentum parameter
            start: 1,           # TODO
            saturate: 250,      # TODO
            final_momentum: .7  # TODO
        }
    ]
}
