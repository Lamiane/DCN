# on convolutinal networks: /home/nex/Aplikacje/pylearn2/pylearn2/scripts/tutorials/convolutional_network
!obj:pylearn2.train.Train { # our experiment
    dataset: &train !obj:pylearn2.datasets.mnist.MNIST { # our data set will be a MNIST
        which_set: 'train', # 'train' or 'test' TODO: ???
        start: 0, # starting index
        stop: %(train_stop)i # last index
    },
    model: !obj:pylearn2.models.mlp.MLP { # we will train a multilayer perceptron
        layers: [ # the list of layers of the network
                 !obj:pylearn2.models.mlp.Sigmoid { # sigmoid layer
                     layer_name: 'h0', # it's name, it can be anything
                     dim: %(dim_h0)i, # dimensions of this layer
                     sparse_init: 15, # "If specified, initial sparse_init number of weights for each unit
                                      # from N(0,1)." TODO: I don't get it
                 }, !obj:pylearn2.models.mlp.Softmax { # softmax layer (like a fancy sigmoid)
                     layer_name: 'y', # name of the layer
                     n_classes: 10, # number of classes (which is also the size of the layer)
                     irange: 0. # "If specified, initialized each weight randomly in U(-irange, irange)."
                 }
                ],
        nvis: 784, # "Number of “visible units” (input units). Equivalent to specifying
                   # input_space=VectorSpace(dim=nvis)."
    },
    algorithm: !obj:pylearn2.training_algorithms.bgd.BGD { # trainig algorithm: Batch Gradient Descent
        batch_size: 10000, # just a batch size
        line_search_mode: 'exhaustive', # documentations says nothing about it For the directions to be
                                        # truly conjugate, you must use line_search_mode = ‘exhaustive’
                                        # and the objective function must be quadratic.
                                        # Using line_search_mode = ‘exhaustive’ on a non-quadratic objective
                                        # function implements nonlinear conjugate gradient descent.
        conjugate: 1, # "Passed through to the optimization.BatchGradientDescent’s conjugate parameter"
                      # If True, tries to pick conjugate gradient directions.
        updates_per_batch: 10,  # Passed through to the optimization.BatchGradientDescent’s max_iters parameter
                                # The maximum number of iterations to run, per code vector, if the optimization
                                # has still not converged. Default is 1000. TODO: what does it mean?
        monitoring_dataset: # A Dataset or a dictionary mapping string dataset names to Datasets
            {
                'train' : *train, # train set
                'valid' : !obj:pylearn2.datasets.mnist.MNIST {
                              which_set: 'train', # refers to MNIST train set
                              start: 50000, # first index
                              stop: %(valid_stop)i # last index
                          },
                'test'  : !obj:pylearn2.datasets.mnist.MNIST {
                              which_set: 'test', # refers to MNIST test set
                          }
            },
        termination_criterion: !obj:pylearn2.termination_criteria.And { # Keep learning until any of a set of criteria
                                                                        # wants to stop. Termination criterion
                                                                        # representing the logical conjunction of
                                                                        # several individual criteria. Optimization
                                                                        # continues only if every constituent criterion
                                                                        # returns True.
            criteria: [ # A sequence of callables representing termination criteria, with a return value of True
                        # indicating that training should continue.
                !obj:pylearn2.termination_criteria.MonitorBased {
                    channel_name: "valid_y_misclass"    # Name of the channel to examine. If None and the monitor has
                                                        # only one channel, this channel will be used; otherwise,
                                                        # an error will be raised. For more info on monitors
                                                        # check the pylearn2 documentation
                },
                !obj:pylearn2.termination_criteria.EpochCounter {   # Learn for a fixed number of epochs.
                                                                    # A termination criterion that uses internal
                                                                    # state to trigger termination after a fixed
                                                                    # number of calls (epochs).
                    max_epochs: %(max_epochs)i # after how many epochs learning shall stop
                }
            ]
        }
    },
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {   # A callback that saves a copy of the
                                                                            # model every time it achieves a new
                                                                            # minimal value of a monitoring channel.
                                                                            # Also stores the best model in memory.
             channel_name: 'valid_y_misclass', # monitoring channel name
             save_path: "%(save_path)s/mlp_best.pkl" # here the learned model will be stored
        },
    ]
}
